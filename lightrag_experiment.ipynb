{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823c8bc-ee76-467e-b821-3c0dd0456840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "\n",
    "#########\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Insert API Key\"\n",
    "\n",
    "#########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdba75f-ae2f-4bab-a74f-8df158f9b887",
   "metadata": {},
   "source": [
    "## Graph Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627da0b-5f9c-49aa-961f-458f30c026cb",
   "metadata": {},
   "source": [
    "### - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f66cfdc-b738-4e5d-a35f-41bbfb7af013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "knowledge_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"knowledge_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1bd9f92d370>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load the GraphML file\n",
    "G = nx.read_graphml('./llama_3_quant_cs/graph_chunk_entity_relation.graphml')\n",
    "\n",
    "# Create a Pyvis network\n",
    "net = Network(notebook=True, height='750px', width='100%')\n",
    "\n",
    "# Color map for nodes\n",
    "color_map = {\n",
    "    'type1': 'darkblue',\n",
    "    'type2': 'blue',\n",
    "    'type3': 'green',\n",
    "    'default': 'gray'\n",
    "}\n",
    "\n",
    "# Add nodes with color and size customization\n",
    "for node in G.nodes(data=True):\n",
    "    category = node[1].get('category', 'type1')\n",
    "    color = color_map.get(category, 'darkgrey')\n",
    "    degree = G.degree(node[0])\n",
    "    net.add_node(\n",
    "        node[0], \n",
    "        label=node[1].get('label', node[0]), \n",
    "        color=color, \n",
    "        size=(degree**1/3) + 15\n",
    "    )\n",
    "\n",
    "# Add edges with customization\n",
    "for source, target, edge_data in G.edges(data=True):\n",
    "    weight = edge_data.get('weight', 1)\n",
    "    width = 2 if weight < 5 else 5\n",
    "    net.add_edge(source, target, width=width)\n",
    "\n",
    "# Set physics options for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -20000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 95\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"cubicBezier\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save and display the network\n",
    "net.show('knowledge_graph.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38516fa-f684-4602-8862-ac6f65d80b4a",
   "metadata": {},
   "source": [
    "### - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ede24b-4be2-4d17-9518-c10ba896fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "knowledge_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"knowledge_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1bdf3c4aba0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load the GraphML file\n",
    "G = nx.read_graphml('./gpt_book/graph_chunk_entity_relation.graphml')\n",
    "\n",
    "# Create a Pyvis network\n",
    "net = Network(notebook=True)\n",
    "\n",
    "# Convert NetworkX graph to Pyvis network\n",
    "net.from_nx(G)\n",
    "\n",
    "# Save and display the network\n",
    "net.show('knowledge_graph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb4344-7843-4231-98ef-9858a2233f01",
   "metadata": {},
   "source": [
    "## Auth to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225d6b4-8741-4ddb-a816-18011884028f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n",
    "token = \"<Insert Token>\"  # Replace with your actual token\n",
    "\n",
    "# Use token to load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token, trust_remote_code=True)\n",
    "\n",
    "print(\"Successfully loaded the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf12916-0e02-468f-a854-8ccdb881e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "print(f\"OpenAI library version: {openai.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22329905-1f78-4d88-ba5e-b78e90fac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b382da-62c2-48dd-8c22-42cb96fae347",
   "metadata": {},
   "source": [
    "## Process Input Document Files and Generate Knowledge Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0b45d-5e8f-4009-a60b-2c06901ac44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import psutil\n",
    "import GPUtil\n",
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<insert api>\"\n",
    "\n",
    "\n",
    "#########\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Determine the device to use\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# List of BASE values to represent multiple datasets\n",
    "BASE_LIST = [\"cs\", \"legal\", \"mix\", \"agriculture\"]\n",
    "\n",
    "# Set your Hugging Face token\n",
    "HF_TOKEN = \"<HF_TOKEN>\"  # Replace with your actual token\n",
    "\n",
    "# Initialize LightRAG with Hugging Face model\n",
    "from lightrag.llm import hf_model_complete, hf_embedding\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "# Model information\n",
    "MODEL_NAME = 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4'\n",
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Define the function to preprocess the text\n",
    "def preprocess_text(file_path, output_path, max_lines=100):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        count = 0\n",
    "        for line in infile:\n",
    "            # Stop after max_lines lines for faster processing\n",
    "            if count >= max_lines:\n",
    "                break\n",
    "            \n",
    "            # Remove numbered artifacts like _100, _115, etc.\n",
    "            cleaned_line = re.sub(r'_\\d+\\n?', '', line)\n",
    "            # Replace 3 or more consecutive newlines with 2 newlines\n",
    "            cleaned_line = re.sub(r'\\n{3,}', '\\n\\n', cleaned_line)\n",
    "            # Strip leading/trailing whitespace\n",
    "            cleaned_line = cleaned_line.strip()\n",
    "            # Write the cleaned line to the output file\n",
    "            outfile.write(cleaned_line + \"\\n\")\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace escaped newlines (\\\\n) with actual newlines\n",
    "    cleaned_text = text.replace('\\\\n', '\\n')\n",
    "    # Replace multiple newlines with a maximum of two\n",
    "    cleaned_text = re.sub(r'\\\\n{2,}', '\\n\\n', cleaned_text)\n",
    "        # Replace multiple newlines with a maximum of two\n",
    "    cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', cleaned_text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Load tokenizer and embedding model, and move the model to GPU\n",
    "#tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "#embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME).to(device)\n",
    "\n",
    "# Iterate over each base in the list\n",
    "for BASE in BASE_LIST:\n",
    "    # Define paths dynamically using BASE\n",
    "    WORKING_DIR = f\"./gpt_{BASE}\"\n",
    "    LOG_DIR = os.path.join(WORKING_DIR, \"logs\")\n",
    "    LOG_FILE = os.path.join(LOG_DIR, f\"insertion_log_gpt_{BASE}.txt\")\n",
    "    FILE = f\"./{BASE}.jsonl\"\n",
    "    CLEANED_FILE = f\"./cleaned_{BASE}.jsonl\"\n",
    "\n",
    "    # Check if the working directory exists, if not, create it\n",
    "    if not os.path.exists(WORKING_DIR):\n",
    "        os.makedirs(WORKING_DIR)\n",
    "        print(f\"Created working directory: {WORKING_DIR}\")\n",
    "    else:\n",
    "        print(f\"Working directory already exists: {WORKING_DIR}\")\n",
    "\n",
    "    # Preprocess the text to remove artifacts and store only 100 lines for testing purposes\n",
    "    try:\n",
    "        preprocess_text(FILE, CLEANED_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {FILE} not found. Skipping to the next BASE.\")\n",
    "        continue\n",
    "\n",
    "    # Initialize LightRAG with Hugging Face model\n",
    "    rag_gpt = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=gpt_4o_mini_complete  # Use gpt_4o_mini_complete LLM model\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Start time measurement before processing the file\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Open and read the cleaned JSON lines file\n",
    "    try:\n",
    "        X = 100 #Read 100 documents; each document is a line in the input file\n",
    "        with open(FILE, encoding=\"utf-8\") as f:\n",
    "            count = 0  # Initialize line counter\n",
    "            for line in f:\n",
    "                # Stop after reading X lines for faster processing\n",
    "                if count >= X:\n",
    "                    break\n",
    "                \n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line.strip())\n",
    "\n",
    "                # Extract fields: input, context, answers, etc.\n",
    "                input_text = data.get(\"input\", \"\")\n",
    "                context = data.get(\"context\", \"\")\n",
    "\n",
    "                # Clean up input and context\n",
    "                input_text = clean_text(input_text)\n",
    "                context = clean_text(context)\n",
    "\n",
    "                # Construct the text to be inserted into the knowledge graph\n",
    "                combined_text = f\"Input: {input_text}\\nContext: {context}\"\n",
    "\n",
    "                # Insert into LightRAG\n",
    "                rag_gpt.insert(combined_text)\n",
    "\n",
    "                # Increment the line counter\n",
    "                count += 1\n",
    "                print(\"Successfully read a full line\")\n",
    "                print(count)\n",
    "\n",
    "        # End time measurement after processing all lines\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate and log the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Collect CPU and Memory usage stats\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)  # CPU usage percentage\n",
    "        memory_info = psutil.virtual_memory()  # Memory stats\n",
    "        memory_usage = memory_info.percent  # Memory usage percentage\n",
    "\n",
    "        # Collect GPU usage stats (if applicable)\n",
    "        gpu_status = \"N/A\"\n",
    "        #if device == \"cuda\":\n",
    "        #    gpus = GPUtil.getGPUs()\n",
    "        #    if gpus:\n",
    "        #        gpu = gpus[0]  # Assuming one GPU\n",
    "        #        gpu_status = f\"GPU ID: {gpu.id}, GPU Load: {gpu.load * 100:.2f}%, VRAM Used: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB, Temperature: {gpu.temperature}Â°C\"\n",
    "        \n",
    "        # Prepare log information\n",
    "        log_data = f\"\"\"\n",
    "        ==== Data Insertion Log ====\n",
    "        Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n",
    "        Base: {BASE}\n",
    "        Device Used: {device}\n",
    "        Working Directory: {WORKING_DIR}\n",
    "        Model Name: {MODEL_NAME}\n",
    "        Embedding Model Name: {EMBEDDING_MODEL_NAME}\n",
    "        Hugging Face Token Used: {'Provided' if HF_TOKEN else 'Not Provided'}\n",
    "        Total Time Taken for Insertion: {elapsed_time:.2f} seconds\n",
    "        Total Number of Entries Processed: {count}\n",
    "        CPU Usage: {cpu_usage:.2f}%\n",
    "        Memory Usage: {memory_usage:.2f}%\n",
    "        ===========================\n",
    "        \"\"\"\n",
    "\n",
    "        # Print log data to console\n",
    "        print(log_data)\n",
    "\n",
    "        # Write log data to a file\n",
    "        with open(LOG_FILE, \"a\") as log_file:\n",
    "            log_file.write(log_data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {CLEANED_FILE} not found. Skipping to the next BASE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf88202-0c5f-4e1c-8d6d-e28dae51bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def log_query_response(base, query, response, log_dir=\"./query_logs\"):\n",
    "    # Ensure log directory exists\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Construct the log file path\n",
    "    log_file = os.path.join(log_dir, f\"{base}_responses.json\")\n",
    "\n",
    "    # Load existing responses (if any)\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            logged_data = json.load(f)\n",
    "    else:\n",
    "        logged_data = {}\n",
    "\n",
    "    # Append the new response\n",
    "    logged_data[query] = response\n",
    "\n",
    "    # Write back to the file\n",
    "    with open(log_file, \"w\") as f:\n",
    "        json.dump(logged_data, f, indent=4)\n",
    "\n",
    "    print(f\"Response for {base} logged successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20089e6-c431-45c1-a7e2-e2ea911c34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import psutil\n",
    "#import GPUtil  # For GPU usage stats\n",
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the list of knowledge graph domains\n",
    "BASE_LIST = [\"cs\", \"legal\", \"mix\", \"agriculture\"]\n",
    "\n",
    "# Function to query the knowledge graph\n",
    "def query_knowledge_graph(rag_model, query, graph_name):\n",
    "    print(f\"Querying with knowledge graph: {graph_name}\")\n",
    "    \n",
    "    # Configure query parameters\n",
    "    params = QueryParam(query=query)\n",
    "\n",
    "    # Perform the query\n",
    "    response = rag_model.query(params)\n",
    "    \n",
    "    # Return the query response\n",
    "    return response\n",
    "\n",
    "# Function to log responses\n",
    "def log_query_response(base, query, model_name, response, log_dir=\"./query_logs\"):\n",
    "    # Ensure log directory exists\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Construct the log file path\n",
    "    log_file = os.path.join(log_dir, f\"{base}_responses.json\")\n",
    "\n",
    "    # Load existing responses (if any)\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            logged_data = json.load(f)\n",
    "    else:\n",
    "        logged_data = {}\n",
    "\n",
    "    # Add response under the model name\n",
    "    if query not in logged_data:\n",
    "        logged_data[query] = {}\n",
    "    logged_data[query][model_name] = response\n",
    "\n",
    "    # Write back to the file\n",
    "    with open(log_file, \"w\") as f:\n",
    "        json.dump(logged_data, f, indent=4)\n",
    "\n",
    "    print(f\"Response for {base} query '{query}' logged successfully.\")\n",
    "\n",
    "# Define queries and user profiles\n",
    "users = {\n",
    "    \"agriculture\": \"Agricultural Researcher\",\n",
    "    \"cs\": \"Data Scientist\",\n",
    "    \"legal\": \"Corporate Legal Advisor\",\n",
    "    \"mix\": \"Literary Scholar\"\n",
    "}\n",
    "\n",
    "queries = {\n",
    "    \"agriculture\": [\n",
    "        \"What are the best practices for hive management?\",\n",
    "        \"What are the most common challenges faced by beekeepers when managing hive health?\",\n",
    "        \"How do crop production practices influence bee populations and hive productivity?\",\n",
    "        \"What methods are most effective for disease prevention in hive management?\",\n",
    "        \"What role does honey extraction and straining play in ensuring product quality and sustainability?\",\n",
    "        \"How can agricultural practices be adapted to mitigate the impact of climate change on pollination?\"\n",
    "    ],\n",
    "    \"cs\": [\n",
    "        \"Explain recommendation systems in machine learning.\",\n",
    "        \"What are the key features of Spark Streaming that make it suitable for real-time analytics?\",\n",
    "        \"How do classification algorithms differ when applied to recommendation systems versus real-time analytics?\",\n",
    "        \"What challenges arise when processing large-scale data sets for recommendation systems, and how are they addressed?\",\n",
    "        \"How can Spark's architecture optimize the handling of streaming data in real-time scenarios?\",\n",
    "        \"What are the trade-offs between batch processing and real-time analytics in data science applications?\"\n",
    "    ],\n",
    "    \"legal\": [\n",
    "        \"What is the primary purpose of a Restructuring Support Agreement, and how is it typically negotiated?\",\n",
    "        \"What legal considerations are critical during corporate governance restructuring?\",\n",
    "        \"How does regulatory compliance vary across industries during restructuring?\",\n",
    "        \"What are the potential risks and mitigations in legal agreements related to restructuring?\",\n",
    "        \"How do financial sector regulations influence corporate restructuring processes?\"\n",
    "    ],\n",
    "    \"mix\": [\n",
    "        \"What is the historical impact of skepticism in philosophy?\",\n",
    "        \"What is the main objection Mary has to the poem 'The Witch of Atlas,' and how does it reflect her literary perspective?\",\n",
    "        \"How do the philosophical themes in 'The Witch of Atlas' relate to broader historical trends in literature?\",\n",
    "        \"What cultural influences are evident in the poem 'The Witch of Atlas,' and how do they shape its interpretation?\",\n",
    "        \"How do objections to specific works contribute to literary debates on artistic merit and intention?\",\n",
    "        \"What role does biographical context play in interpreting objections to 'The Witch of Atlas'?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over each domain and each model\n",
    "models = {\n",
    "    \"llama_3_quant\": gpt_4o_mini_complete,\n",
    "    \"gpt\": gpt_4o_mini_complete # Replace with the actual GPT model function\n",
    "}\n",
    "\n",
    "for BASE in BASE_LIST:\n",
    "    for model_name, model_func in models.items():\n",
    "        # Define working directory for the current knowledge graph\n",
    "        WORKING_DIR = f\"./{model_name}_{BASE}\"\n",
    "        \n",
    "        # Initialize the LightRAG model with the corresponding knowledge graph\n",
    "        #embedding_func = get_embedding_function(WORKING_DIR)\n",
    "        rag_model = LightRAG(working_dir=WORKING_DIR,\n",
    "                             llm_model_func=model_func)\n",
    "        \n",
    "        # Perform each query for the domain\n",
    "        for query in queries[BASE]:\n",
    "            response = query_knowledge_graph(rag_model, query, BASE)\n",
    "            print(f\"Query: {query}\\nResponse ({model_name}): {response}\\n\")\n",
    "            \n",
    "            # Log the response\n",
    "            log_query_response(BASE, query, model_name, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ca56-9083-42ba-90a6-794b8e7e081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Function to call the API for evaluation\n",
    "def evaluate_responses(query, answer1, answer2):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "    }\n",
    "\n",
    "    # Construct evaluation prompt\n",
    "    instruction_prompt = \"\"\"\n",
    "    You are an expert tasked with evaluating two answers to the same question based on three criteria: Comprehensiveness, Diversity, and Empowerment.\n",
    "    For each criterion, give a grade (1 to 5), choose the better answer, and explain why. Then, select an overall winner based on these criteria.\n",
    "    \"\"\"\n",
    "    input_prompt = f\"\"\"\n",
    "    Here is the question: {query}\n",
    "\n",
    "    Here are the two answers:\n",
    "    Answer 1: {answer1}\n",
    "    Answer 2: {answer2}\n",
    "\n",
    "    Evaluate both answers using the criteria and provide detailed explanations for each.\n",
    "    Output your evaluation in the following JSON format:\n",
    "    {{\n",
    "      \"Comprehensiveness\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Diversity\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Empowerment\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Overall Winner\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Summary]\", \"Grades\": [1, 2]}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # API request payload\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": instruction_prompt},\n",
    "            {\"role\": \"user\", \"content\": input_prompt}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data)\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        evaluation = response.json()['choices'][0]['message']['content']\n",
    "        return evaluation\n",
    "    else:\n",
    "        raise ValueError(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b5c1-c41c-4a83-9ab5-663c589081bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Function to calculate graph metrics\n",
    "def calculate_graph_metrics(graph_path):\n",
    "    # Load the graph\n",
    "    G = nx.read_graphml(graph_path)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
    "    connected_components = nx.number_connected_components(G)\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        \"num_nodes\": num_nodes,\n",
    "        \"num_edges\": num_edges,\n",
    "        \"avg_degree\": avg_degree,\n",
    "        \"connected_components\": connected_components\n",
    "    }\n",
    "BASE_LIST = [\"cs\", \"legal\", \"mix\", \"agriculture\"]\n",
    "# Compare graphs for each domain\n",
    "graph_metrics = {}\n",
    "for BASE in BASE_LIST:\n",
    "    for model in [\"llama_3_quant\", \"gpt\"]:\n",
    "        graph_path = f\"./{model}_{BASE}/graph_chunk_entity_relation.graphml\"\n",
    "        try:\n",
    "            metrics = calculate_graph_metrics(graph_path)\n",
    "            graph_metrics[f\"{model}_{BASE}\"] = metrics\n",
    "            print(f\"Metrics for {model}_{BASE}: {metrics}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Graph file not found for {model}_{BASE}\")\n",
    "\n",
    "# Example output visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "metrics_df = pd.DataFrame.from_dict(graph_metrics, orient='index')\n",
    "metrics_df.index.name = 'Model_Domain'\n",
    "\n",
    "# Display metrics as a table\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize the metrics\n",
    "metrics_df.plot(kind='bar', figsize=(12, 6), subplots=True, layout=(2, 2), title=\"Graph Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c2f5d-981b-48cc-b133-8a8e2b08083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Function to call the API for evaluation\n",
    "def evaluate_responses(query, answer1, answer2):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "    }\n",
    "\n",
    "    # Construct evaluation prompt\n",
    "    instruction_prompt = \"\"\"\n",
    "    You are an expert tasked with evaluating two answers to the same question based on three criteria: Comprehensiveness, Diversity, and Empowerment.\n",
    "    For each criterion, give a grade (1 to 5), choose the better answer, and explain why. Then, select an overall winner based on these criteria.\n",
    "    \"\"\"\n",
    "    input_prompt = f\"\"\"\n",
    "    Here is the question: {query}\n",
    "\n",
    "    Here are the two answers:\n",
    "    Answer 1: {answer1}\n",
    "    Answer 2: {answer2}\n",
    "\n",
    "    Evaluate both answers using the criteria and provide detailed explanations for each.\n",
    "    Output your evaluation in the following JSON format:\n",
    "    {{\n",
    "      \"Comprehensiveness\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Diversity\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Empowerment\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Explanation]\", \"Grades\": [1, 2]}},\n",
    "      \"Overall Winner\": {{\"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Summary]\", \"Grades\": [1, 2]}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # API request payload\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": instruction_prompt},\n",
    "            {\"role\": \"user\", \"content\": input_prompt}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data)\n",
    "\n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        evaluation = response.json()['choices'][0]['message']['content']\n",
    "        return evaluation\n",
    "    else:\n",
    "        raise ValueError(f\"API request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba78b1-dfeb-4886-91a6-f7c659e1bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from lightrag import EmbeddingFunc\n",
    "\n",
    "# Queries to be used for each knowledge graph domain\n",
    "queries = {\n",
    "    \"agriculture\": [\n",
    "        \"What are the best practices for hive management?\",\n",
    "        \"What are the most common challenges faced by beekeepers when managing hive health?\",\n",
    "        \"How do crop production practices influence bee populations and hive productivity?\",\n",
    "        \"What methods are most effective for disease prevention in hive management?\",\n",
    "        \"What role does honey extraction and straining play in ensuring product quality and sustainability?\",\n",
    "        \"How can agricultural practices be adapted to mitigate the impact of climate change on pollination?\"\n",
    "    ],\n",
    "    \"cs\": [\n",
    "        \"Explain recommendation systems in machine learning.\",\n",
    "        \"What are the key features of Spark Streaming that make it suitable for real-time analytics?\",\n",
    "        \"How do classification algorithms differ when applied to recommendation systems versus real-time analytics?\",\n",
    "        \"What challenges arise when processing large-scale data sets for recommendation systems, and how are they addressed?\",\n",
    "        \"How can Spark's architecture optimize the handling of streaming data in real-time scenarios?\",\n",
    "        \"What are the trade-offs between batch processing and real-time analytics in data science applications?\"\n",
    "    ],\n",
    "    \"legal\": [\n",
    "        \"What is the primary purpose of a Restructuring Support Agreement, and how is it typically negotiated?\",\n",
    "        \"What legal considerations are critical during corporate governance restructuring?\",\n",
    "        \"How does regulatory compliance vary across industries during restructuring?\",\n",
    "        \"What are the potential risks and mitigations in legal agreements related to restructuring?\",\n",
    "        \"How do financial sector regulations influence corporate restructuring processes?\"\n",
    "    ],\n",
    "    \"mix\": [\n",
    "        \"What is the historical impact of skepticism in philosophy?\",\n",
    "        \"What is the main objection Mary has to the poem 'The Witch of Atlas,' and how does it reflect her literary perspective?\",\n",
    "        \"How do the philosophical themes in 'The Witch of Atlas' relate to broader historical trends in literature?\",\n",
    "        \"What cultural influences are evident in the poem 'The Witch of Atlas,' and how do they shape its interpretation?\",\n",
    "        \"How do objections to specific works contribute to literary debates on artistic merit and intention?\",\n",
    "        \"What role does biographical context play in interpreting objections to 'The Witch of Atlas'?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Models to be used\n",
    "models = {\n",
    "    \"llama_3_quant\": {\n",
    "        \"llm_model_func\": hf_model_complete,\n",
    "        \"llm_model_name\": 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
    "        \"embedding_func\": EmbeddingFunc(\n",
    "            embedding_dim=384,\n",
    "            max_token_size=5000,\n",
    "            func=lambda texts: AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")(\n",
    "                **AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")(texts, return_tensors='pt', padding=True)\n",
    "            )\n",
    "        )\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"llm_model_func\": gpt_4o_mini_complete,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to initialize LightRAG model for a given domain and model\n",
    "def initialize_model(model_name, domain):\n",
    "    working_dir = f\"./{model_name}_{domain}\"\n",
    "    model_details = models[model_name]\n",
    "\n",
    "    # For Llama, additional parameters are passed\n",
    "    if model_name == \"llama_3_quant\":\n",
    "        return LightRAG(\n",
    "            working_dir=working_dir,\n",
    "            llm_model_func=model_details[\"llm_model_func\"],\n",
    "            llm_model_name=model_details[\"llm_model_name\"],\n",
    "            embedding_func=model_details[\"embedding_func\"]\n",
    "        )\n",
    "    # For GPT, it's simpler\n",
    "    else:\n",
    "        return LightRAG(\n",
    "            working_dir=working_dir,\n",
    "            llm_model_func=model_details[\"llm_model_func\"]\n",
    "        )\n",
    "\n",
    "# DataFrame to store the results\n",
    "columns = [\"domain\", \"query\", \"answer_gpt\", \"answer_llama\"]\n",
    "responses_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through each domain and query both models\n",
    "for base, query_list in queries.items():\n",
    "    # Initialize models for the given domain\n",
    "    model_gpt = initialize_model(\"gpt\", base)\n",
    "    model_llama = initialize_model(\"llama_3_quant\", base)\n",
    "\n",
    "    # Perform each query for the current models and domain\n",
    "    for query in query_list:\n",
    "        # Query GPT model\n",
    "        response_gpt = model_gpt.query(QueryParam(query=query))\n",
    "        \n",
    "        # Query Llama model\n",
    "        response_llama = \"\"\n",
    "        #response_llama = model_llama.query(QueryParam(query=query))\n",
    "\n",
    "        # Append the response to the DataFrame\n",
    "        responses_df = responses_df.append({\n",
    "            \"domain\": base,\n",
    "            \"query\": query,\n",
    "            \"answer_gpt\": response_gpt,\n",
    "            \"answer_llama\": response_llama\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file for easy comparison\n",
    "output_file = \"./responses_comparison.csv\"\n",
    "responses_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All queries executed and responses saved successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e133fe9-1de7-4337-be5a-3e357b168e05",
   "metadata": {},
   "source": [
    "## Iterate over list of domains for GPT Model and log responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0facba-ea91-4013-bc82-8de291bcc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lightrag import LightRAG, QueryParam\n",
    "\n",
    "# Queries to be used for each knowledge graph domain\n",
    "queries = {\n",
    "    \"agriculture\": [\n",
    "        \"What are the best practices for hive management?\",\n",
    "        \"What are the most common challenges faced by beekeepers when managing hive health?\",\n",
    "        \"How do crop production practices influence bee populations and hive productivity?\",\n",
    "        \"What methods are most effective for disease prevention in hive management?\",\n",
    "        \"What role does honey extraction and straining play in ensuring product quality and sustainability?\",\n",
    "        \"How can agricultural practices be adapted to mitigate the impact of climate change on pollination?\"\n",
    "    ],\n",
    "    \"cs\": [\n",
    "        \"Explain recommendation systems in machine learning.\",\n",
    "        \"What are the key features of Spark Streaming that make it suitable for real-time analytics?\",\n",
    "        \"How do classification algorithms differ when applied to recommendation systems versus real-time analytics?\",\n",
    "        \"What challenges arise when processing large-scale data sets for recommendation systems, and how are they addressed?\",\n",
    "        \"How can Spark's architecture optimize the handling of streaming data in real-time scenarios?\",\n",
    "        \"What are the trade-offs between batch processing and real-time analytics in data science applications?\"\n",
    "    ],\n",
    "    \"legal\": [\n",
    "        \"What is the primary purpose of a Restructuring Support Agreement, and how is it typically negotiated?\",\n",
    "        \"What legal considerations are critical during corporate governance restructuring?\",\n",
    "        \"How does regulatory compliance vary across industries during restructuring?\",\n",
    "        \"What are the potential risks and mitigations in legal agreements related to restructuring?\",\n",
    "        \"How do financial sector regulations influence corporate restructuring processes?\"\n",
    "    ],\n",
    "    \"mix\": [\n",
    "        \"What is the historical impact of skepticism in philosophy?\",\n",
    "        \"What is the main objection Mary has to the poem 'The Witch of Atlas,' and how does it reflect her literary perspective?\",\n",
    "        \"How do the philosophical themes in 'The Witch of Atlas' relate to broader historical trends in literature?\",\n",
    "        \"What cultural influences are evident in the poem 'The Witch of Atlas,' and how do they shape its interpretation?\",\n",
    "        \"How do objections to specific works contribute to literary debates on artistic merit and intention?\",\n",
    "        \"What role does biographical context play in interpreting objections to 'The Witch of Atlas'?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Models to be used\n",
    "gpt_model = {\n",
    "    \"llm_model_func\": gpt_4o_mini_complete,\n",
    "}\n",
    "\n",
    "# Function to initialize LightRAG model for a given domain and model\n",
    "def initialize_gpt_model(domain):\n",
    "    working_dir = f\"./gpt_{domain}\"\n",
    "    return LightRAG(\n",
    "        working_dir=working_dir,\n",
    "        llm_model_func=gpt_model[\"llm_model_func\"]\n",
    "    )\n",
    "\n",
    "# DataFrame to store the results\n",
    "columns = [\"domain\", \"query\", \"answer_gpt\", \"answer_llama\"]\n",
    "responses_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Iterate through each domain and query only the GPT model\n",
    "for base, query_list in queries.items():\n",
    "    # Initialize GPT model for the given domain\n",
    "    model_gpt = initialize_gpt_model(base)\n",
    "\n",
    "    # Perform each query for the current model and domain\n",
    "    for query in query_list:\n",
    "        # Query GPT model\n",
    "        response_gpt = model_gpt.query(query=query)\n",
    "        \n",
    "        # Append the response to the DataFrame\n",
    "        responses_df = pd.concat([responses_df, pd.DataFrame([{\n",
    "            \"domain\": base,\n",
    "            \"query\": query,\n",
    "            \"answer_gpt\": response_gpt,\n",
    "            \"answer_llama\": \"\"  # Leave Llama responses empty for now\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file for easy comparison\n",
    "output_file = \"./responses_comparison.csv\"\n",
    "responses_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All queries executed and responses saved successfully to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08973619-fd63-4454-ba3d-49bd6f57d7d2",
   "metadata": {},
   "source": [
    "## Iterate over list of domains for Quantized Model and log responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29847f-7aec-4657-a497-473849d13157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from lightrag import EmbeddingFunc\n",
    "\n",
    "# Load the DataFrame generated previously\n",
    "input_file = \"./responses_comparison.csv\"\n",
    "\n",
    "responses_df = pd.read_csv(input_file)\n",
    "\n",
    "# Model to be used for Llama\n",
    "llama_model = {\n",
    "    \"llm_model_func\": hf_model_complete,\n",
    "    \"llm_model_name\": 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
    "    \"embedding_func\": EmbeddingFunc(\n",
    "        embedding_dim=384,\n",
    "        max_token_size=5000,\n",
    "        func=lambda texts: AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")(\n",
    "            **AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")(texts, return_tensors='pt', padding=True)\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Function to initialize LightRAG model for a given domain and model\n",
    "def initialize_llama_model(domain):\n",
    "    working_dir = f\"./llama_3_quant_{domain}\"\n",
    "    return LightRAG(\n",
    "        working_dir=working_dir,\n",
    "        llm_model_func=llama_model[\"llm_model_func\"],\n",
    "        llm_model_name=llama_model[\"llm_model_name\"],\n",
    "        embedding_func=llama_model[\"embedding_func\"]\n",
    "    )\n",
    "\n",
    "# Iterate over the DataFrame and fill in the Llama responses\n",
    "for index, row in responses_df.iterrows():\n",
    "    if pd.isna(row['answer_llama']) or row['answer_llama'] == \"\":\n",
    "        # Initialize Llama model for the given domain\n",
    "        model_llama = initialize_llama_model(row['domain'])\n",
    "        \n",
    "        # Query Llama model\n",
    "        response_llama = model_llama.query(query=row['query'])\n",
    "        \n",
    "        # Update the DataFrame with the Llama response\n",
    "        responses_df.at[index, 'answer_llama'] = response_llama\n",
    "\n",
    "# Save the updated DataFrame to the CSV file\n",
    "responses_df.to_csv(input_file, index=False)\n",
    "\n",
    "print(f\"All Llama queries executed and responses updated successfully in {input_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3cc22-0d01-4a8d-b91c-4fe2a6dc6336",
   "metadata": {},
   "source": [
    "## Evaluation of generated document quality with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a2213-b27d-43b8-8767-463080a863f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Load the CSV file containing the responses\n",
    "responses_df = pd.read_csv(\"./responses_comparison_new.csv\")\n",
    "\n",
    "# Log to save the evaluations\n",
    "evaluation_log = []\n",
    "\n",
    "# Function to generate the evaluation prompt\n",
    "def generate_evaluation_prompt(query, answer1, answer2):\n",
    "    return f\"\"\"\n",
    "    Evaluation Instruction Prompt\n",
    "\n",
    "    You are an expert tasked with evaluating two answers to the same question based on four criteria: Comprehensiveness, Diversity, and Empowerment.\n",
    "\n",
    "    Goal:\n",
    "    You will evaluate two answers to the same question based on our criteria: Comprehensiveness, Diversity, and Empowerment.\n",
    "\n",
    "    Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question?\n",
    "    Diversity: How varied and rich is the answer in providing different perspectives and insights on the question?\n",
    "    Empowerment: How well does the answer help the reader understand and make informed judgments about the topic?\n",
    "\n",
    "    For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories and also grade each answer.\n",
    "\n",
    "    Evaluation Input Prompt\n",
    "\n",
    "    Here is the question: {query}\n",
    "\n",
    "    Here are the two answers: Answer 1: {answer1}; Answer 2: {answer2}\n",
    "\n",
    "    Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.\n",
    "\n",
    "    Output your evaluation in the following JSON format:\n",
    "\n",
    "    {{\n",
    "      \"Comprehensiveness\": {{ \"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Provide explanation here]\", \"Grade1\": X, \"Grade2\": Y }},\n",
    "      \"Diversity\": {{ \"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Provide explanation here]\", \"Grade1\": X, \"Grade2\": Y }},\n",
    "      \"Empowerment\": {{ \"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Provide explanation here]\", \"Grade1\": X, \"Grade2\": Y }},\n",
    "      \"Overall Winner\": {{ \"Winner\": \"[Answer 1 or Answer 2]\", \"Explanation\": \"[Summarize why this answer is the overall winner based on the three criteria]\", \"Grade1\": X, \"Grade2\": Y }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "# Function to call the evaluation API using OpenAI's latest API\n",
    "def gpt_evaluate(prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=1000,  # Increased max_tokens for a detailed response\n",
    "            temperature=0.7  # Adjust as per your needs for more control or creativity\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Iterate over each row in the DataFrame and evaluate the responses\n",
    "for index, row in responses_df.iterrows():\n",
    "    query = row['query']\n",
    "    answer_gpt = row['answer_gpt']\n",
    "    answer_llama = row['answer_llama']\n",
    "\n",
    "    # Generate the evaluation prompt\n",
    "    evaluation_prompt = generate_evaluation_prompt(query, answer_gpt, answer_llama)\n",
    "\n",
    "    # Get evaluation response\n",
    "    evaluation_result = gpt_evaluate(evaluation_prompt)\n",
    "\n",
    "    # Proceed only if evaluation_result is not None\n",
    "    if evaluation_result is None:\n",
    "        continue\n",
    "\n",
    "    # Convert the JSON string response to a dictionary\n",
    "    try:\n",
    "        evaluation_data = json.loads(evaluation_result)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON for query at index {index}: {evaluation_result}\")\n",
    "        continue\n",
    "\n",
    "    # Add domain, query, and evaluation data to the log\n",
    "    log_entry = {\n",
    "        \"domain\": row['domain'],\n",
    "        \"query\": query,\n",
    "        \"evaluation\": evaluation_data\n",
    "    }\n",
    "    evaluation_log.append(log_entry)\n",
    "\n",
    "# Convert the evaluation log into a structured DataFrame for summary\n",
    "evaluation_summary = []\n",
    "for entry in evaluation_log:\n",
    "    domain = entry['domain']\n",
    "    query = entry['query']\n",
    "    evaluation = entry['evaluation']\n",
    "\n",
    "    # Extract grades and overall winner\n",
    "    gpt_grade = evaluation['Overall Winner']['Grade1']\n",
    "    llama_grade = evaluation['Overall Winner']['Grade2']\n",
    "    overall_winner = evaluation['Overall Winner']['Winner']\n",
    "\n",
    "    # Add to summary list\n",
    "    evaluation_summary.append({\n",
    "        \"domain\": domain,\n",
    "        \"query\": query,\n",
    "        \"gpt_grade\": gpt_grade,\n",
    "        \"llama_grade\": llama_grade,\n",
    "        \"overall_winner\": overall_winner\n",
    "    })\n",
    "\n",
    "# Create DataFrame from evaluation summary\n",
    "summary_df = pd.DataFrame(evaluation_summary)\n",
    "\n",
    "# Save the evaluation summary to a CSV file\n",
    "summary_output_file = \"./evaluation_summary.csv\"\n",
    "summary_df.to_csv(summary_output_file, index=False)\n",
    "\n",
    "# Save detailed log to a JSON file\n",
    "detailed_log_file = \"./evaluation_log.json\"\n",
    "with open(detailed_log_file, \"w\") as f:\n",
    "    json.dump(evaluation_log, f, indent=4)\n",
    "\n",
    "print(f\"Evaluation summary saved to {summary_output_file}. Detailed log saved to {detailed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839cab5-96db-4aab-88d3-880cc6df1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for comparison\n",
    "responses = {\n",
    "    \"agriculture\": {\n",
    "        \"llama_3\": \"Response from llama_3 for agriculture\",\n",
    "        \"gpt\": \"Response from gpt for agriculture\"\n",
    "    },\n",
    "    # Add other domain responses...\n",
    "}\n",
    "\n",
    "for domain, answers in responses.items():\n",
    "    query = queries[domain]\n",
    "    answer1 = answers[\"llama_3\"]\n",
    "    answer2 = answers[\"gpt\"]\n",
    "\n",
    "    print(f\"Evaluating for domain: {domain}\")\n",
    "    evaluation = evaluate_responses(query, answer1, answer2)\n",
    "    print(f\"Evaluation Results:\\n{evaluation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908d79f-9e68-45ec-b400-1fffe89bd68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LightRAG-env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
